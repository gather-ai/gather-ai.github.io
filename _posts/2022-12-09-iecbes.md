---
title: "IECBES 2022 - Conference Highlights"
subtitle: "First Time Attending a Conference Physically"
date: 2022-12-09
categories: 
  - Summaries
tags: 
  - AI Research
  - Conference
header: 
  image: "/assets/images/ezdet/object-detection.jpg"
toc: true
toc_sticky: true
---

ðŸ‘‹ Hi there. Welcome back to my page. In the last 2 tutorial series on [Domain Generalization](https://gather-ai.github.io/tutorials/domain-generalization-part-1/) and [Federated Learning on IoT Devices](https://gather-ai.github.io/tutorials/federated-learning-iot-part-1/), we dealt with 2 different types of classification (ECG classification and image classification), the most fundamental (and simple) task in Machine Learning (ML). Today, we will explore how to easily move from classification to object detection, a more advanced task in ML and Computer Vision (CV). Let's get started. 

## 1. Background

### Motivation
I start this tutorial series and an open-source repository [ezdet](https://github.com/lhkhiem28/ezdet) by 3 observations: 
* When people begin to learn ML, specifically CV, they typically begin with an image classification tutorial, such as [PyTorch's one](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html). After that, they usually move to the object detection problem next, where the difficulty occurs. In particular, the tutorial on object detection in the community is not good and dissimilar from the one on classification. Therefore, people come to some open-source repositories like [YOLO](https://github.com/ultralytics/yolov5) from Ultralytics or [Detectron](https://github.com/facebookresearch/detectron2). Although these repositories are powerful, they still have their own drawbacks. 
* Available open-source repositories are very complex and equipped with many advanced techniques. This makes them not good starting points for people who just come to the term and want to understand object detection in a similar way as classification. Moreover, equipping many add-on techniques makes it difficult to fairly compare object detection models to each other. 
* These wonderful repositories are designed in a way that is not flexible enough for engineers and researchers to integrate object detection models into other ML projects. 

With the above observations, I created [ezdet](https://github.com/lhkhiem28/ezdet) to overcome these issues. Firstly, the ezdetâ€™s source code is organized in a similar way to the classification problem. Secondly, ezdet decouples the standard object detection process with other add-on techniques. Finally, ezdet can be easily integrated into other ML projects. At the end of this tutorial, we will embed ezdet into a Federated Learning project. 

### Object Detection
Object detection is an advanced task in the CV field that deals with the localization and classification of objects contained in an image or video. For easy understanding, letâ€™s distinguish between image classification and object detection. Image classification sends a whole image through a classifier for it to spit out a tag. Classifiers take into consideration the whole image but donâ€™t tell you where the tag appears in the image. Object detection is slightly more sophisticated, as it creates a bounding box around the classified object. Figure 1 illustrates this distinction. 

<figure class="align-center">
  <img src="{{ site.url }}{{ site.baseurl }}/assets/images/ezdet/classification-vs-detection.jpg">
  <figcaption>Figure 1. Image Classification vs. Object Detection. Mount from [1]</figcaption>
</figure>

From an ML perspective, object detection is a multi-task learning problem, the term that we discussed in a [previous article](https://gather-ai.github.io/tutorials/domain-generalization-part-2/). Specifically, the detectors are trained with a joined (simplified) objective function as below: 

$$\mathcal{L}_{total} = \lambda_{loc}\mathcal{L}_{loc}(\widehat{b}, b) + \lambda_{cls}\mathcal{L}_{cls}(\widehat{y}, y)$$

where $\widehat{b}$ and $b$ are predicted and ground truth bounding box coordinates, usually in the form of ($x_{min}$, $y_{min}$, $x_{max}$, $y_{max}$) or ($x_{center}$, $y_{center}$, $width$, $height$); $\widehat{y}$ and $y$ are the predicted probability and ground truth category of the object in that bounding box. $$\mathcal{L}_{loc}$$ can be a simple IoU function, $$\mathcal{L}_{cls}$$ can be a cross-entropy loss function; $\lambda_{loc}$ and $\lambda_{cls}$ are control hyper-parameters to balance these two loss terms. 

Object detection models typically can be categorized into 2 groups: 
* Two-stage (Proposal-based) detectors: The two stages of a two-stage detector can be divided by an RoI (Region of Interest) Pooling layer. One of the prominent two-stage object detectors is [Faster R-CNN](https://arxiv.org/abs/1506.01497). It has the first stage called RPN, a Region Proposal Network to predict candidate bounding boxes. In the second stage, features are by RoI pooling operation from each candidate box for the following classification and bounding box regression tasks. 
* One-stage (Proposal-free) detectors: In contrast, a one-stage detector predicts bounding boxes in a single step without using region proposals. It leverages the help of a grid box and anchors to localize the region of detection in the image and constraint the shape of the object. In this tutorial, I will use a [YOLOv3](https://arxiv.org/abs/1804.02767) model, a popular one-stage detector, with the [API](https://github.com/eriklindernoren/PyTorch-YOLOv3) implemented in PyTorch for demonstration, other architecture will be developed in the future. 

## References
[[1] The Ultimate Guide to Object Detection](https://www.v7labs.com/blog/object-detection-guide)<br>
{: style="font-size: 14px;"}
---
title: "Easy Object Detection - Part 1"
subtitle: "From Classification to Object Detection with YOLO"
date: 2022-11-19
categories: 
  - Tutorials
tags: 
  - AI Research
  - Deep Learning
  - Computer Vision
header: 
  image: "/assets/images/ezdet/object-detection.jpg"
toc: true
toc_sticky: true
---

ðŸ‘‹ Hi there. Welcome back to my page. In the last 2 tutorial series on [Domain Generalization](https://gather-ai.github.io/tutorials/domain-generalization-part-1/) and [Federated Learning on IoT Devices](https://gather-ai.github.io/tutorials/federated-learning-iot-part-1/), we dealt with 2 different types of classification (ECG classification and image classification), the most fundamental (and simple) task in Machine Learning (ML). Today, we will explore how to easily move from classification to object detection, a more advanced task in ML and Computer Vision (CV). Let's get started. 

## 1. Background

### Motivation
I start this tutorial series and an open-source repository [ezdet](https://github.com/lhkhiem28/ezdet) by 3 observations: 
* When people begin to learn ML, specifically CV, they typically begin with an image classification tutorial, such as [PyTorch's one](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html). After that, they usually move to the object detection problem next, where the difficulty occurs. In particular, the tutorial on object detection in the community is not good and dissimilar from the one on classification. Therefore, people come to some open-source repositories like [YOLO](https://github.com/ultralytics/yolov5) from Ultralytics or [Detectron](https://github.com/facebookresearch/detectron2). Although these repositories are powerful, they still have their own drawbacks. 
* Available open-source repositories are very complex and equipped with many advanced techniques. This makes them not good starting points for people who just come to the term and want to understand object detection in a similar way as classification. Moreover, equipping many add-on techniques makes it difficult to fairly compare object detection models to each other. 
* These wonderful repositories are designed in a way that is not flexible enough for engineers and researchers to integrate object detection models into other ML projects. 

With the above observations, I created [ezdet](https://github.com/lhkhiem28/ezdet) to overcome these issues. Firstly, the ezdetâ€™s source code is organized in a similar way to the classification problem. Secondly, ezdet decouples the standard object detection process with other add-on techniques. Finally, ezdet can be easily integrated into other ML projects. At the end of this tutorial, we will embed ezdet into a Federated Learning project. 

### Object Detection
Object detection is an advanced task in the CV field that deals with the localization and classification of objects contained in an image or video. For easy understanding, letâ€™s distinguish between image classification and object detection. Image classification sends a whole image through a classifier for it to spit out a tag. Classifiers take into consideration the whole image but donâ€™t tell you where the tag appears in the image. Object detection is slightly more sophisticated, as it creates a bounding box around the classified object. Figure 1 illustrates this distinction. 

<figure class="align-center">
  <img src="{{ site.url }}{{ site.baseurl }}/assets/images/ezdet/classification-vs-detection.jpg">
  <figcaption>Figure 1. Image Classification vs. Object Detection. Mount from [1]</figcaption>
</figure>

From an ML perspective, object detection is a multi-task learning problem, the term that we discussed in a [previous article](https://gather-ai.github.io/tutorials/domain-generalization-part-2/). Specifically, the detectors are trained with a joined (simplified) objective function as below: 

$$\mathcal{L}_{total} = \lambda_{loc}\mathcal{L}_{loc}(\widehat{b}, b) + \lambda_{cls}\mathcal{L}_{cls}(\widehat{y}, y)$$

where $\widehat{b}$ and $b$ are predicted and ground truth bounding box coordinates, usually in the form of ($x_{min}$, $y_{min}$, $x_{max}$, $y_{max}$) or ($x_{center}$, $y_{center}$, $width$, $height$); $\widehat{y}$ and $y$ are the predicted probability and ground truth category of the object in that bounding box. $$\mathcal{L}_{loc}$$ can be a simple IoU function, $$\mathcal{L}_{cls}$$ can be a cross-entropy loss function; $\lambda_{loc}$ and $\lambda_{cls}$ are control hyper-parameters to balance these two loss terms. 

Object detection models typically can be categorized into 2 groups: 
* Two-stage (Proposal-based) detectors: The two stages of a two-stage detector can be divided by an RoI (Region of Interest) Pooling layer. One of the prominent two-stage object detectors is [Faster R-CNN](https://arxiv.org/abs/1506.01497). It has the first stage called RPN, a Region Proposal Network to predict candidate bounding boxes. In the second stage, features are by RoI pooling operation from each candidate box for the following classification and bounding box regression tasks. 
* One-stage (Proposal-free) detectors: In contrast, a one-stage detector predicts bounding boxes in a single step without using region proposals. It leverages the help of a grid box and anchors to localize the region of detection in the image and constraint the shape of the object. In this tutorial, I will use a [YOLOv3](https://arxiv.org/abs/1804.02767) model, a popular one-stage detector, implemented in [PyTorch](https://github.com/eriklindernoren/PyTorch-YOLOv3) for demonstration, other architecture will be developed in the future. 

## 2. Building an Object Detection Pipeline
In this section, we will walk through an end-to-end object detection pipeline, which is built in a similar way to any Pytorch classification pipeline. 

### VOC2007 Dataset
The VOC2007 dataset will be used, which contains 5011 images and 4952 images in the training set and test set, respectively. You can download the dataset from [here](https://pjreddie.com/projects/pascal-voc-dataset-mirror/). After downloading, you should organize the dataset as the below structure and then convert the provided labels to YOLO format. 

```
â”‚
â”œâ”€â”€â”€datasets
â”‚   â””â”€â”€â”€VOC2007
â”‚       â”œâ”€â”€â”€train
â”‚       â”‚   â”œâ”€â”€â”€images
â”‚       â”‚   â”‚       2007_000005.jpg
â”‚       â”‚   â”‚       ...
â”‚       â”‚   â””â”€â”€â”€labels
â”‚       â”‚           2007_000005.xml
â”‚       â”‚           ...
â”‚       â””â”€â”€â”€val
â”‚           â”œâ”€â”€â”€images
â”‚           â”‚       2007_000001.jpg
â”‚           â”‚       ...
â”‚           â””â”€â”€â”€labels
â”‚                   2007_000001.xml
|                   ...
â”œâ”€â”€â”€source
â”‚   â””â”€â”€â”€*.py
```

After organizing the dataset, as any Pytorch classification pipeline, we need to write a `Dataset` class with a `__getitem__` function to return a pair of an image and its label (bounding boxes and classes). Snippet 1 illustrates the implementation. 

```python
"""
Snippet 1: Dataset class. 
"""
from libs import *

class DetImageDataset(torch.utils.data.Dataset):
    def __init__(self, 
        images_path, labels_path
        , image_size = 416
        , augment = False
        , multiscale = False
    ):
        self.image_files, self.label_files = sorted(glob.glob(images_path + "/*")), sorted(glob.glob(labels_path + "/*"))
        self.image_size = image_size
        self.augment = augment
        self.transforms = A.Compose(
            [
                A.HorizontalFlip(
                    p = 0.5, 
                ), 
                A.BBoxSafeRandomCrop(
                    erosion_rate = 0.2, 
                    p = 0.5, 
                ), 
                A.RandomBrightnessContrast(
                    brightness_limit = 0.2, contrast_limit = 0.2, 
                    p = 0.3, 
                ), 
                A.RGBShift(
                    r_shift_limit = 30, g_shift_limit = 30, b_shift_limit = 30, 
                    p = 0.3, 
                ), 
            ], 
            A.BboxParams("yolo", ["classes"])
        )
        self.multiscale = multiscale
        self.image_sizes = [self.image_size + 32*scale for scale in range(-1, 2)]

    def __len__(self, 
    ):
        return len(self.image_files)

    def square_pad(self, 
        image, 
    ):
        _, h, w = image.shape
        gap_pad = np.abs(h - w)
        if h - w < 0:
            pad = (0, 0, gap_pad // 2, gap_pad - gap_pad // 2)
        else:
            pad = (gap_pad // 2, gap_pad - gap_pad // 2, 0, 0)

        image = F.pad(
            image, 
            pad = pad, value = 0.0, 
        )
        return image, [0] + list(pad)

    def __getitem__(self, 
        index, 
    ):
        image_file, label_file = self.image_files[index], self.label_files[index]
        image = cv2.imread(image_file)
        image = cv2.cvtColor(
            image, 
            code = cv2.COLOR_BGR2RGB, 
        )
        bboxes = np.loadtxt(label_file)
        bboxes = bboxes.reshape(-1, 5)
        if self.augment:
            Transformed = self.transforms(
                image = image, 
                classes = bboxes[:, 0], bboxes = bboxes[:, 1:]
            )
            image = Transformed["image"]
            bboxes[:, 1:] = np.array(Transformed["bboxes"])

        image = torch.tensor(image)
        image = image.permute(2, 0, 1)
        _, h, w = image.shape
        image, pad = self.square_pad(image); _, padded_h, padded_w = image.shape
        c1, c2, c3, c4,  = w*(bboxes[:, 1] - bboxes[:, 3]/2) + pad[1], w*(bboxes[:, 1] + bboxes[:, 3]/2) + pad[2], h*(bboxes[:, 2] - bboxes[:, 4]/2) + pad[3], h*(bboxes[:, 2] + bboxes[:, 4]/2) + pad[4], 
        bboxes[:, 1], bboxes[:, 2], bboxes[:, 3], bboxes[:, 4],  = ((c1 + c2)/2)/padded_w, ((c3 + c4)/2)/padded_h, bboxes[:, 3]*(w/padded_w), bboxes[:, 4]*(h/padded_h), 
        return image.float(), F.pad(
            torch.tensor(bboxes), 
            pad = (1, 0, 0, 0), value = 0.0, 
        )

    def collate_fn(self, 
        batch, 
    ):
        images, labels = list(zip(*batch))
        if self.multiscale and np.random.random() <= 0.1:
            self.image_size = np.random.choice(self.image_sizes)
        images = torch.stack([
            F.interpolate(
                image.unsqueeze(0), 
                self.image_size, mode = "nearest", 
            ).squeeze(0) for image in images
        ])
        images = images/255

        labels = [bboxes for bboxes in labels if bboxes is not None]
        for index, bboxes in enumerate(labels):
            bboxes[:, 0] = index
        if len(labels) != 0:labels = torch.cat(labels)
        return images, labels
```

The component that I want you to notice in the above implementation is `self.transforms`, which is used for performing data augmentation. Here, I use the [Albumentations](https://albumentations.ai/) library, you can modify the `self.transforms` attribute to use the data augmentation strategy that you want depending on your problem. 

### Config the model

## References
[[1] The Ultimate Guide to Object Detection](https://www.v7labs.com/blog/object-detection-guide)<br>
[[2] Bibliometric Analysis of One-stage and Two-stage Object Detection](https://www.researchgate.net/publication/349297260_Bibliometric_Analysis_of_One-stage_and_Two-stage_Object_Detection)<br>
{: style="font-size: 14px;"}